== [[SparkEnv]] `SparkEnv` -- Spark Runtime Environment

*Spark Runtime Environment* (`SparkEnv`) is the runtime environment with Spark's public services that interact with each other to establish a distributed computing platform for a Spark application.

Spark Runtime Environment is represented by a <<SparkEnv, SparkEnv>> object that holds all the required runtime services for a running Spark application with separate environments for the <<createDriverEnv, driver>> and <<createExecutorEnv, executors>>.

The idiomatic way in Spark to access the current `SparkEnv` when on the driver or executors is to use <<get, get>> method.

[source, scala]
----
import org.apache.spark._
scala> SparkEnv.get
res0: org.apache.spark.SparkEnv = org.apache.spark.SparkEnv@49322d04
----

.`SparkEnv` Services
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|======================
| Property | Service | Description
| [[serializer]] serializer | Serializer |
| [[closureSerializer]] closureSerializer | Serializer |
| serializerManager | SerializerManager |
| [[mapOutputTracker]] mapOutputTracker | link:spark-service-mapoutputtracker.adoc[MapOutputTracker] |
| [[shuffleManager]] shuffleManager | link:spark-shuffle-manager.adoc[ShuffleManager] |
| [[broadcastManager]] broadcastManager | BroadcastManager |
| [[blockManager]] <<BlockManager, blockManager>> | link:spark-blockmanager.adoc[BlockManager] |
| securityManager | SecurityManager |
| metricsSystem | MetricsSystem |
| memoryManager | MemoryManager |
| outputCommitCoordinator | OutputCommitCoordinator |
| [[conf]] conf | link:spark-configuration.adoc[SparkConf]
|======================

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.SparkEnv` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.SparkEnv=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[SparkEnv]] `SparkEnv` Factory Object

=== [[create]] Creating "Base" `SparkEnv` -- `create` Method

[source, scala]
----
create(
  conf: SparkConf,
  executorId: String,
  hostname: String,
  port: Int,
  isDriver: Boolean,
  isLocal: Boolean,
  numUsableCores: Int,
  listenerBus: LiveListenerBus = null,
  mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv
----

`create` is a internal helper method to create a "base" `SparkEnv` regardless of the target environment, i.e. a driver or an executor.

.``create``'s Input Arguments and Their Usage
[frame="topbot",cols="1,2",options="header",width="100%"]
|======================
| Input Argument | Usage
| `bindAddress` | Used to create link:spark-rpc.adoc[RpcEnv] and link:spark-NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService].

| `advertiseAddress` | Used to create link:spark-rpc.adoc[RpcEnv] and link:spark-NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService].

| `numUsableCores` | Used to create link:spark-MemoryManager.adoc[MemoryManager], link:spark-NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService] and link:spark-blockmanager.adoc#creating-instance[BlockManager].
|======================

When executed, `create` creates a `Serializer` (based on <<spark_serializer, spark.serializer>> setting). You should see the following `DEBUG` message in the logs:

```
DEBUG SparkEnv: Using serializer: [serializer]
```

It creates another `Serializer` (based on <<spark_closure_serializer, spark.closure.serializer>>).

It creates a link:spark-shuffle-manager.adoc[ShuffleManager] based on link:spark-shuffle-manager.adoc#spark.shuffle.manager[spark.shuffle.manager] setting.

[[MemoryManager]]
It creates a link:spark-MemoryManager.adoc[MemoryManager] based on <<spark_memory_useLegacyMode, spark.memory.useLegacyMode>> setting (with link:spark-UnifiedMemoryManager.adoc[UnifiedMemoryManager] being the default and `numCores` the input `numUsableCores`).

[[NettyBlockTransferService]]
`create` creates a link:spark-NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService]. It uses link:spark-driver.adoc#spark_driver_blockManager_port[`spark.driver.blockManager.port` for the port on the driver] and link:spark-blockmanager.adoc#spark_blockManager_port[`spark.blockManager.port` for the port on executors].

CAUTION: FIXME A picture with `SparkEnv`, `NettyBlockTransferService` and the ports "armed".

[[BlockManagerMaster]]
`create` creates a link:spark-BlockManagerMaster.adoc#creating-instance[BlockManagerMaster] object with the `BlockManagerMaster` RPC endpoint reference (by <<registerOrLookupEndpoint, registering or looking it up by name>> and link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMasterEndpoint]), the input link:spark-configuration.adoc[SparkConf], and the input `isDriver` flag.

.Creating BlockManager for the Driver
image::images/sparkenv-driver-blockmanager.png[align="center"]

NOTE: `create` registers the *BlockManagerMaster* RPC endpoint for the driver and looks it up for executors.

.Creating BlockManager for Executor
image::images/sparkenv-executor-blockmanager.png[align="center"]

[[BlockManager]]
It creates a link:spark-blockmanager.adoc#creating-instance[BlockManager] (using the above <<BlockManagerMaster, BlockManagerMaster>>, <<NettyBlockTransferService, NettyBlockTransferService>> and other services).

It creates a link:spark-service-broadcastmanager.adoc[BroadcastManager].

It creates a CacheManager.

It creates a MetricsSystem for a driver and a worker separately.

It initializes `userFiles` temporary directory used for downloading dependencies for a driver while this is the executor's current working directory for an executor.

An OutputCommitCoordinator is created.

NOTE: `create` is called by <<createDriverEnv, createDriverEnv>> and <<createExecutorEnv, createExecutorEnv>>.

=== [[registerOrLookupEndpoint]] Registering or Looking up RPC Endpoint by Name -- `registerOrLookupEndpoint` Method

[source, scala]
----
registerOrLookupEndpoint(name: String, endpointCreator: => RpcEndpoint)
----

`registerOrLookupEndpoint` registers or looks up a RPC endpoint by `name`.

If called from the driver, you should see the following INFO message in the logs:

```
INFO SparkEnv: Registering [name]
```

And the RPC endpoint is registered in the RPC environment.

Otherwise, it obtains a RPC endpoint reference by `name`.

=== [[createDriverEnv]] Creating SparkEnv for Driver -- `createDriverEnv` Method

[source, scala]
----
createDriverEnv(
  conf: SparkConf,
  isLocal: Boolean,
  listenerBus: LiveListenerBus,
  numCores: Int,
  mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv
----

`createDriverEnv` creates a `SparkEnv` execution environment for the driver.

.Spark Environment for driver
image::images/sparkenv-driver.png[align="center"]

`createDriverEnv` accepts an instance of link:spark-configuration.adoc[SparkConf], link:spark-deployment-environments.adoc[whether it runs in local mode or not], link:spark-LiveListenerBus.adoc[LiveListenerBus], the number of cores to use for execution in local mode or `0` otherwise, and a link:spark-service-outputcommitcoordinator.adoc[OutputCommitCoordinator] (default: none).

`createDriverEnv` ensures that link:spark-driver.adoc#spark_driver_host[spark.driver.host] and <<spark_driver_port, spark.driver.port>> Spark properties are defined.

It then passes the call straight on to the <<create, create helper method>> (with `driver` executor id, `isDriver` enabled, and the input parameters).

NOTE: `createDriverEnv` is exclusively used by link:spark-sparkcontext-creating-instance-internals.adoc#createSparkEnv[SparkContext to create a `SparkEnv`] (while a link:spark-sparkcontext.adoc#creating-instance[SparkContext is being created for the driver]).

=== [[createExecutorEnv]] Creating SparkEnv for Executor -- `createExecutorEnv` Method

[source, scala]
----
createExecutorEnv(
  conf: SparkConf,
  executorId: String,
  hostname: String,
  port: Int,
  numCores: Int,
  isLocal: Boolean): SparkEnv
----

`createExecutorEnv` creates an *executor's (execution) environment* that is the Spark execution environment for an executor.

.Spark Environment for executor
image::images/sparkenv-executor.png[align="center"]

`createExecutorEnv` uses link:spark-configuration.adoc[SparkConf], the executor's identifier, hostname, port, the number of cores to use, and whether or not it runs in local mode.

NOTE: The number of cores to use is configured using `--cores` command-line option of `CoarseGrainedExecutorBackend` and is specific to a cluster manager.

It creates an link:spark-service-mapoutputtracker.adoc#MapOutputTrackerWorker[MapOutputTrackerWorker] object and looks up `MapOutputTracker` RPC endpoint. See link:spark-service-mapoutputtracker.adoc[MapOutputTracker].

It creates a MetricsSystem for *executor* and starts it.

An OutputCommitCoordinator is created and *OutputCommitCoordinator* RPC endpoint looked up.

=== [[stop]] `stop` Method

CAUTION: FIXME

=== [[get]] Getting Current `SparkEnv` -- `get` Method

[source, scala]
----
get: SparkEnv
----

`get` returns the current `SparkEnv`.

[source, scala]
----
import org.apache.spark._
scala> SparkEnv.get
res0: org.apache.spark.SparkEnv = org.apache.spark.SparkEnv@49322d04
----

=== [[settings]] Settings

.Spark Properties
[frame="topbot",cols="1,1,2",options="header",width="100%"]
|======================
| Spark Property | Default Value | Description

| [[spark_driver_port]] `spark.driver.port` | `0` | The port the driver listens to. It is first set to `0` in the driver when link:spark-sparkcontext.adoc#creating-instance[SparkContext is initialized]. It is later set to the port of link:spark-rpc.adoc[RpcEnv] of the driver (in <<create, SparkEnv.create>>).
| [[spark_serializer]] `spark.serializer` | `org.apache.spark.serializer.JavaSerializer` | The `Serializer`.

[TIP]
====
Enable DEBUG logging level for `org.apache.spark.SparkEnv` logger to see the current value.

```
DEBUG SparkEnv: Using serializer: [serializer]
```
====

| [[spark_closure_serializer]] `spark.closure.serializer` | `org.apache.spark.serializer.JavaSerializer` | The `Serializer`

| [[spark_memory_useLegacyMode]] `spark.memory.useLegacyMode` | `false` | Controls what type of the link:spark-MemoryManager.adoc[MemoryManager] to use. When enabled (`true`) it is the legacy `StaticMemoryManager` while link:spark-UnifiedMemoryManager.adoc[UnifiedMemoryManager] otherwise (`false`).
|======================

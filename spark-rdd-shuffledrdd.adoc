== [[ShuffledRDD]] ShuffledRDD

`ShuffledRDD` is an link:spark-rdd.adoc[RDD] of `(key, value)` pairs across <<ShuffledRDDPartition, ShuffledRDDPartition>> partitions. It is a shuffle step (the result RDD) for transformations that trigger link:spark-rdd-shuffle.adoc[shuffle] at execution.

`ShuffledRDD` requires a parent RDD and a link:spark-rdd-Partitioner.adoc[Partitioner] when created.

NOTE: `ShuffledRDD` is a result of link:spark-rdd-partitions.adoc#coalesce[`coalesce` transformation] (when `shuffle` is enabled), link:spark-rdd-pairrdd-functions.adoc#combineByKeyWithClassTag[combineByKeyWithClassTag] (when partitioners are different), link:spark-rdd-pairrdd-functions.adoc#partitionBy[partitionBy] (using a different `Partitioner`), link:spark-rdd-OrderedRDDFunctions.adoc#sortByKey[sortByKey] and link:spark-rdd-OrderedRDDFunctions.adoc#repartitionAndSortWithinPartitions[repartitionAndSortWithinPartitions] ordered operators.

By default, the map-side combining flag (`mapSideCombine`) is disabled (i.e. `false`). It can however be changed using `ShuffledRDD.setMapSideCombine(mapSideCombine: Boolean)` method (and is used in link:spark-rdd-pairrdd-functions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] that sets it `true` by default).

`getDependencies` returns a single-element collection of link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency].

Let's have a look at the below example with `groupBy` transformation:

```
scala> val r = sc.parallelize(0 to 9, 3).groupBy(_ / 3)
r: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:18

scala> r.toDebugString
res0: String =
(3) ShuffledRDD[2] at groupBy at <console>:18 []
 +-(3) MapPartitionsRDD[1] at groupBy at <console>:18 []
    |  ParallelCollectionRDD[0] at parallelize at <console>:18 []
```

As you may have noticed, `groupBy` transformation adds `ShuffledRDD` RDD that will execute shuffling at execution time (as depicted in the following screenshot).

.Two stages in a job due to shuffling
image::images/spark-webui-job-two-stages.png[align="center"]

It uses link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] to get preferred locations for a shuffle, i.e. a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency].

=== [[compute]] Computing Partition (in `TaskContext`) -- `compute` Method

[source, scala]
----
compute(split: Partition, context: TaskContext): Iterator[(K, C)]
----

NOTE: `compute` is a part of link:spark-rdd.adoc#contract[RDD contract] to compute a given partition in a link:spark-taskscheduler-taskcontext.adoc[TaskContext].

Internally, `compute` makes sure that the input `split` is a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]. It then link:spark-shuffle-manager.adoc#contract[requests `ShuffleManager` for a `ShuffleReader`] to read key-value pairs (as `Iterator[(K, C)]`) for the `split`.

NOTE: `compute` uses link:spark-sparkenv.adoc#shuffleManager[`SparkEnv` to access `ShuffleManager`].

NOTE: A Partition has the `index` property to specify `startPartition` and `endPartition` partition offsets.

=== [[getPreferredLocations]] `getPreferredLocations` Method

[source, scala]
----
getPreferredLocations(partition: Partition): Seq[String]
----

NOTE: `getPreferredLocations` is a part of link:spark-rdd.adoc#contract[RDD contract] to specify placement preferences (aka _preferred task locations_), i.e. where tasks should be executed to be as close to the data as possible.

Internally, `getPreferredLocations` requests link:spark-service-MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[`MapOutputTrackerMaster` for the preferred locations] of the input `partition` (for the only link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]).

NOTE: `getPreferredLocations` uses link:spark-sparkenv.adoc#mapOutputTracker[`SparkEnv` to access `MapOutputTrackerMaster`] (which runs on the driver).

=== [[ShuffledRDDPartition]] ShuffledRDDPartition

`ShuffledRDDPartition` gets an `index` when it is created (that in turn is the index of partitions as calculated by the link:spark-rdd-Partitioner.adoc[Partitioner] of a <<ShuffledRDD, ShuffledRDD>>).
